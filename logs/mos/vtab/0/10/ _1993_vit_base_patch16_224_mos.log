2024-01-27 21:42:23,729 [trainer.py] => config: ./exps/mos_vtab.json
2024-01-27 21:42:23,729 [trainer.py] => prefix:  
2024-01-27 21:42:23,729 [trainer.py] => dataset: vtab
2024-01-27 21:42:23,729 [trainer.py] => memory_size: 0
2024-01-27 21:42:23,729 [trainer.py] => memory_per_class: 0
2024-01-27 21:42:23,729 [trainer.py] => fixed_memory: False
2024-01-27 21:42:23,729 [trainer.py] => shuffle: False
2024-01-27 21:42:23,729 [trainer.py] => init_cls: 10
2024-01-27 21:42:23,729 [trainer.py] => increment: 10
2024-01-27 21:42:23,729 [trainer.py] => model_name: mos
2024-01-27 21:42:23,729 [trainer.py] => backbone_type: vit_base_patch16_224_mos
2024-01-27 21:42:23,729 [trainer.py] => device: [device(type='cuda', index=3)]
2024-01-27 21:42:23,729 [trainer.py] => seed: 1993
2024-01-27 21:42:23,729 [trainer.py] => tuned_epoch: 20
2024-01-27 21:42:23,729 [trainer.py] => init_lr: 0.03
2024-01-27 21:42:23,729 [trainer.py] => batch_size: 32
2024-01-27 21:42:23,729 [trainer.py] => weight_decay: 0.005
2024-01-27 21:42:23,729 [trainer.py] => min_lr: 0
2024-01-27 21:42:23,729 [trainer.py] => optimizer: sgd
2024-01-27 21:42:23,729 [trainer.py] => scheduler: cosine
2024-01-27 21:42:23,730 [trainer.py] => reinit_optimizer: True
2024-01-27 21:42:23,730 [trainer.py] => init_milestones: [10]
2024-01-27 21:42:23,730 [trainer.py] => init_lr_decay: 0.1
2024-01-27 21:42:23,730 [trainer.py] => reg: 0.1
2024-01-27 21:42:23,730 [trainer.py] => adapter_momentum: 0.1
2024-01-27 21:42:23,730 [trainer.py] => ensemble: True
2024-01-27 21:42:23,730 [trainer.py] => crct_epochs: 30
2024-01-27 21:42:23,730 [trainer.py] => ca_lr: 0.005
2024-01-27 21:42:23,730 [trainer.py] => ca_storage_efficient_method: covariance
2024-01-27 21:42:23,730 [trainer.py] => ca_storage_efficient_method_choices: ['covariance', 'multi-centroid', 'variance']
2024-01-27 21:42:23,730 [trainer.py] => n_centroids: 10
2024-01-27 21:42:23,730 [trainer.py] => pretrained: True
2024-01-27 21:42:23,730 [trainer.py] => drop: 0.0
2024-01-27 21:42:23,730 [trainer.py] => drop_path: 0.0
2024-01-27 21:42:23,730 [trainer.py] => ffn_num: 16
2024-01-27 21:42:23,760 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
2024-01-27 21:42:27,422 [mos.py] => 86,141,426 model total parameters.
2024-01-27 21:42:27,422 [mos.py] => 342,770 model training parameters.
2024-01-27 21:42:27,423 [mos.py] => head.weight: 38400
2024-01-27 21:42:27,423 [mos.py] => head.bias: 50
2024-01-27 21:42:27,423 [mos.py] => cur_adapter.0.down_proj.weight: 12288
2024-01-27 21:42:27,423 [mos.py] => cur_adapter.0.down_proj.bias: 16
2024-01-27 21:42:27,423 [mos.py] => cur_adapter.0.up_proj.weight: 12288
2024-01-27 21:42:27,423 [mos.py] => cur_adapter.0.up_proj.bias: 768
2024-01-27 21:42:27,423 [mos.py] => cur_adapter.1.down_proj.weight: 12288
2024-01-27 21:42:27,423 [mos.py] => cur_adapter.1.down_proj.bias: 16
2024-01-27 21:42:27,423 [mos.py] => cur_adapter.1.up_proj.weight: 12288
2024-01-27 21:42:27,423 [mos.py] => cur_adapter.1.up_proj.bias: 768
2024-01-27 21:42:27,423 [mos.py] => cur_adapter.2.down_proj.weight: 12288
2024-01-27 21:42:27,423 [mos.py] => cur_adapter.2.down_proj.bias: 16
2024-01-27 21:42:27,423 [mos.py] => cur_adapter.2.up_proj.weight: 12288
2024-01-27 21:42:27,423 [mos.py] => cur_adapter.2.up_proj.bias: 768
2024-01-27 21:42:27,423 [mos.py] => cur_adapter.3.down_proj.weight: 12288
2024-01-27 21:42:27,423 [mos.py] => cur_adapter.3.down_proj.bias: 16
2024-01-27 21:42:27,423 [mos.py] => cur_adapter.3.up_proj.weight: 12288
2024-01-27 21:42:27,423 [mos.py] => cur_adapter.3.up_proj.bias: 768
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.4.down_proj.weight: 12288
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.4.down_proj.bias: 16
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.4.up_proj.weight: 12288
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.4.up_proj.bias: 768
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.5.down_proj.weight: 12288
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.5.down_proj.bias: 16
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.5.up_proj.weight: 12288
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.5.up_proj.bias: 768
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.6.down_proj.weight: 12288
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.6.down_proj.bias: 16
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.6.up_proj.weight: 12288
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.6.up_proj.bias: 768
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.7.down_proj.weight: 12288
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.7.down_proj.bias: 16
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.7.up_proj.weight: 12288
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.7.up_proj.bias: 768
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.8.down_proj.weight: 12288
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.8.down_proj.bias: 16
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.8.up_proj.weight: 12288
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.8.up_proj.bias: 768
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.9.down_proj.weight: 12288
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.9.down_proj.bias: 16
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.9.up_proj.weight: 12288
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.9.up_proj.bias: 768
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.10.down_proj.weight: 12288
2024-01-27 21:42:27,424 [mos.py] => cur_adapter.10.down_proj.bias: 16
2024-01-27 21:42:27,425 [mos.py] => cur_adapter.10.up_proj.weight: 12288
2024-01-27 21:42:27,425 [mos.py] => cur_adapter.10.up_proj.bias: 768
2024-01-27 21:42:27,425 [mos.py] => cur_adapter.11.down_proj.weight: 12288
2024-01-27 21:42:27,425 [mos.py] => cur_adapter.11.down_proj.bias: 16
2024-01-27 21:42:27,425 [mos.py] => cur_adapter.11.up_proj.weight: 12288
2024-01-27 21:42:27,425 [mos.py] => cur_adapter.11.up_proj.bias: 768
2024-01-27 21:42:27,425 [trainer.py] => All params: 86141426
2024-01-27 21:42:27,426 [trainer.py] => Trainable params: 342770
2024-01-27 21:42:27,426 [mos.py] => Learning on 0-10
2024-01-27 21:42:52,992 [mos.py] => Task 0, Epoch 20/20 => Loss 0.085, Train_accy 96.83
2024-01-27 21:43:01,588 [mos.py] => the accuracy of the original model:87.4
2024-01-27 21:43:01,590 [trainer.py] => No NME accuracy.
2024-01-27 21:43:01,590 [trainer.py] => CNN: {'total': 93.3, '00-09': 93.3, 'old': 0, 'new': 93.3}
2024-01-27 21:43:01,590 [trainer.py] => CNN top1 curve: [93.3]
2024-01-27 21:43:01,590 [trainer.py] => CNN top5 curve: [99.6]

2024-01-27 21:43:01,590 [trainer.py] => Average Accuracy (CNN): 93.3 

2024-01-27 21:43:01,592 [trainer.py] => All params: 86453427
2024-01-27 21:43:01,593 [trainer.py] => Trainable params: 647090
2024-01-27 21:43:01,594 [mos.py] => Learning on 10-20
2024-01-27 21:43:30,469 [mos.py] => Task 1, Epoch 20/20 => Loss 0.027, Train_accy 99.54
2024-01-27 21:43:38,364 [mos.py] => Task 1, Epoch 30/30 => Loss 0.003, CA_accy 99.94
2024-01-27 21:43:45,958 [mos.py] => the accuracy of the original model:86.43
2024-01-27 21:43:45,961 [trainer.py] => No NME accuracy.
2024-01-27 21:43:45,961 [trainer.py] => CNN: {'total': 91.0, '00-09': 93.1, '10-19': 85.75, 'old': 93.1, 'new': 85.75}
2024-01-27 21:43:45,961 [trainer.py] => CNN top1 curve: [93.3, 91.0]
2024-01-27 21:43:45,962 [trainer.py] => CNN top5 curve: [99.6, 99.14]

2024-01-27 21:43:45,962 [trainer.py] => Average Accuracy (CNN): 92.15 

2024-01-27 21:43:45,963 [trainer.py] => All params: 86765427
2024-01-27 21:43:45,963 [trainer.py] => Trainable params: 951410
2024-01-27 21:43:45,964 [mos.py] => Learning on 20-30
2024-01-27 21:44:15,257 [mos.py] => Task 2, Epoch 20/20 => Loss 0.100, Train_accy 97.39
2024-01-27 21:44:24,223 [mos.py] => Task 2, Epoch 30/30 => Loss 0.002, CA_accy 100.00
2024-01-27 21:44:38,962 [mos.py] => the accuracy of the original model:89.77
2024-01-27 21:44:38,963 [trainer.py] => No NME accuracy.
2024-01-27 21:44:38,963 [trainer.py] => CNN: {'total': 92.49, '00-09': 93.4, '10-19': 85.5, '20-29': 94.42, 'old': 91.14, 'new': 94.42}
2024-01-27 21:44:38,963 [trainer.py] => CNN top1 curve: [93.3, 91.0, 92.49]
2024-01-27 21:44:38,963 [trainer.py] => CNN top5 curve: [99.6, 99.14, 99.29]

2024-01-27 21:44:38,964 [trainer.py] => Average Accuracy (CNN): 92.26333333333334 

2024-01-27 21:44:38,965 [trainer.py] => All params: 87077427
2024-01-27 21:44:38,966 [trainer.py] => Trainable params: 1255730
2024-01-27 21:44:38,967 [mos.py] => Learning on 30-40
2024-01-27 21:45:52,274 [mos.py] => Task 3, Epoch 20/20 => Loss 0.236, Train_accy 94.00
2024-01-27 21:46:03,277 [mos.py] => Task 3, Epoch 30/30 => Loss 0.004, CA_accy 99.97
2024-01-27 21:46:58,013 [mos.py] => the accuracy of the original model:82.27
2024-01-27 21:46:58,016 [trainer.py] => No NME accuracy.
2024-01-27 21:46:58,016 [trainer.py] => CNN: {'total': 92.41, '00-09': 94.1, '10-19': 86.5, '20-29': 94.31, '30-39': 92.19, 'old': 92.91, 'new': 92.19}
2024-01-27 21:46:58,017 [trainer.py] => CNN top1 curve: [93.3, 91.0, 92.49, 92.41]
2024-01-27 21:46:58,017 [trainer.py] => CNN top5 curve: [99.6, 99.14, 99.29, 99.58]

2024-01-27 21:46:58,017 [trainer.py] => Average Accuracy (CNN): 92.30000000000001 

2024-01-27 21:46:58,018 [trainer.py] => All params: 87389427
2024-01-27 21:46:58,019 [trainer.py] => Trainable params: 1560050
2024-01-27 21:46:58,020 [mos.py] => Learning on 40-50
2024-01-27 21:47:19,336 [mos.py] => Task 4, Epoch 20/20 => Loss 0.480, Train_accy 98.88
2024-01-27 21:47:28,826 [mos.py] => Task 4, Epoch 30/30 => Loss 0.004, CA_accy 99.96
2024-01-27 21:48:39,040 [mos.py] => the accuracy of the original model:83.94
2024-01-27 21:48:39,045 [trainer.py] => No NME accuracy.
2024-01-27 21:48:39,047 [trainer.py] => CNN: {'total': 93.31, '00-09': 94.0, '10-19': 86.25, '20-29': 93.91, '30-39': 92.56, '40-49': 100.0, 'old': 92.59, 'new': 100.0}
2024-01-27 21:48:39,047 [trainer.py] => CNN top1 curve: [93.3, 91.0, 92.49, 92.41, 93.31]
2024-01-27 21:48:39,047 [trainer.py] => CNN top5 curve: [99.6, 99.14, 99.29, 99.58, 99.63]

2024-01-27 21:48:39,047 [trainer.py] => Average Accuracy (CNN): 92.50200000000001 

2024-01-27 21:48:39,048 [trainer.py] => Forgetting (CNN): 0.21499999999999986
